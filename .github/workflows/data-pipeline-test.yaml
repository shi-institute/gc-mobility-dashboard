name: "Data Pipeline (PR)"

on:
  pull_request:
    branches:
      - main

jobs:
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [determine_tests_to_run, check_previous_pass]
    if: ${{ fromJSON(needs.check_previous_pass.outputs.successfully_passed) == false }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Set up buildx cache directory
        run: |
          mkdir -p /tmp/.buildx-cache

      - name: Restore Docker build cache
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-docker-${{ hashFiles('./data-pipeline/Dockerfile', './data-pipeline/environment.yaml', './data-pipeline/src/**') }}
          restore-keys: |
            ${{ runner.os }}-docker-

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: ./data-pipeline
          file: ./data-pipeline/Dockerfile
          push: false # don't push the image to the registry
          load: true # instead, load the image into the daemon
          tags: gc-mobility-dashboard-data-pipeline:test
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache,mode=max

      - name: Verify Built Images
        run: |
          docker images

      - name: Save Docker image as artifact
        run: |
          docker save gc-mobility-dashboard-data-pipeline:test -o ./gc-mobility-dashboard-data-pipeline.tar

      - name: Upload Docker image artifact
        uses: actions/upload-artifact@v4
        with:
          name: docker-image-data-pipeline
          path: ./gc-mobility-dashboard-data-pipeline.tar
          retention-days: 1 # set a low retention for temporary artifacts

  determine_tests_to_run:
    name: Determine Tests to Run
    runs-on: ubuntu-latest
    outputs:
      output_hash: ${{ steps.check_changed_dirs.outputs.output_hash }}
      no_changes: ${{ steps.check_changed_dirs.outputs.no_changes }}
      run_greenlink_gtfs: ${{ steps.check_changed_dirs.outputs.run_greenlink_gtfs }}
      run_greenlink_ridership: ${{ steps.check_changed_dirs.outputs.run_greenlink_ridership }}
      run_census_acs_5year: ${{ steps.check_changed_dirs.outputs.run_census_acs_5year }}
      run_replica: ${{ steps.check_changed_dirs.outputs.run_replica }}
      run_geocoder: ${{ steps.check_changed_dirs.outputs.run_geocoder }}
      run_generic_src: ${{ steps.check_changed_dirs.outputs.run_generic_src }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0 # fetch all history to ensure we can find the merge base

      - name: Determine changed directories
        id: check_changed_dirs
        run: |
          # Fetch the base branch to ensure the merge-base can be found
          BASE_REF="${{ github.event.pull_request.base.ref }}"
          if [ -z "$BASE_REF" ]; then
            BASE_REF="main"
          fi
          git fetch origin $BASE_REF:$BASE_REF
          MERGE_BASE=$(git merge-base origin/$BASE_REF HEAD)
          echo "Merge base: $MERGE_BASE"
          echo "HEAD: $(git rev-parse HEAD)"
          echo "Base Ref: $BASE_REF"
          echo "Origin Base Ref SHA: $(git rev-parse origin/$BASE_REF)"

          # Get a list of changed files between the merge base and the head of the PR
          # Filter to only files within data-pipeline/src
          CHANGED_FILES=$(git diff --name-only $(git merge-base origin/$BASE_REF HEAD) HEAD -- ./data-pipeline/src/)

          RUN_GREENLINK_GTFS_TEST="false"
          RUN_CENSUS_ACS_5YEAR_TEST="false"
          RUN_REPLICA_TEST="false"
          RUN_GEOCODER_TEST="false"
          RUN_GENERIC_SRC_TEST="false"

          if [ -n "$CHANGED_FILES" ]; then
            echo "Changed files in data-pipeline/src:"
            echo "$CHANGED_FILES"

            for file in $CHANGED_FILES; do
              if [[ "$file" == "data-pipeline/src/etl/sources/greenlink_gtfs/"* ]]; then
                RUN_GREENLINK_GTFS_TEST="true"
              elif [[ "$file" == "data-pipeline/src/etl/sources/greenlink_ridership/"* ]]; then
                RUN_GREENLINK_RIDERSHIP_TEST="true"
              elif [[ "$file" == "data-pipeline/src/etl/sources/census_acs_5year/"* ]]; then
                RUN_CENSUS_ACS_5YEAR_TEST="true"
              elif [[ "$file" == "data-pipeline/src/etl/sources/replica/"* ]]; then
                RUN_REPLICA_TEST="true"
              elif [[ "$file" == "data-pipeline/src/etl/sources/geocoder/"* ]]; then
                RUN_GEOCODER_TEST="true"
              elif [[ "$file" == "data-pipeline/src/etl/sources/essential_services/"* ]]; then
                RUN_ESSENTIAL_SERVICES_TEST="true"
              elif [[ "$file" == "data-pipeline/src/"* ]] && [[ "$file" != "data-pipeline/src/etl/sources/"* ]]; then
                RUN_GENERIC_SRC_TEST="true"
              fi
            done

            # if the generic source test is true, do not trigger the specific tests
            # because the generic test will cover them
            if [ "$RUN_GENERIC_SRC_TEST" == "true" ]; then
                RUN_GREENLINK_GTFS_TEST="false"
                RUN_GREENLINK_RIDERSHIP_TEST="false"
                RUN_CENSUS_ACS_5YEAR_TEST="false"
                RUN_REPLICA_TEST="false"
                RUN_GEOCODER_TEST="false"
                RUN_ESSENTIAL_SERVICES_TEST="false"
            fi

            NO_CHANGES="false"
          else
            NO_CHANGES="true"
            echo "No changes detected in data-pipeline/src."
          fi

          echo "no_changes=$NO_CHANGES" >> "$GITHUB_OUTPUT"
          echo "run_greenlink_gtfs=$RUN_GREENLINK_GTFS_TEST" >> "$GITHUB_OUTPUT"
          echo "run_greenlink_ridership=$RUN_GREENLINK_RIDERSHIP_TEST" >> "$GITHUB_OUTPUT"
          echo "run_census_acs_5year=$RUN_CENSUS_ACS_5YEAR_TEST" >> "$GITHUB_OUTPUT"
          echo "run_replica=$RUN_REPLICA_TEST" >> "$GITHUB_OUTPUT"
          echo "run_geocoder=$RUN_GEOCODER_TEST" >> "$GITHUB_OUTPUT"
          echo "run_essential_services=$RUN_ESSENTIAL_SERVICES_TEST" >> "$GITHUB_OUTPUT"
          echo "run_generic_src=$RUN_GENERIC_SRC_TEST" >> "$GITHUB_OUTPUT"

          # hash "$GITHUB_OUTPUT" so we can include it in the check_previous_pass/update_pass_variable CURRENT_HASH variable
          OUTPUT_HASH=($(cat "$GITHUB_OUTPUT" | sort | sha1sum | cut -d ' ' -f 1))
          echo "output_hash=$OUTPUT_HASH" >> "$GITHUB_OUTPUT"

          # print the results
          echo "no_changes: $RUN_GREENLINK_GTFS_TEST"
          echo "Run Greenlink GTFS: $RUN_GREENLINK_GTFS_TEST"
          echo "Run Greenlink Ridership: $RUN_GREENLINK_RIDERSHIP_TEST"
          echo "Run Census ACS 5-Year: $RUN_CENSUS_ACS_5YEAR_TEST"
          echo "Run Replica: $RUN_REPLICA_TEST"
          echo "Run Geocoder: $RUN_GEOCODER_TEST"
          echo "Run Essential Services: $RUN_ESSENTIAL_SERVICES_TEST"
          echo "Run all: $RUN_GENERIC_SRC_TEST"

  authenticate_google_big_query:
    name: Authenticate Google BigQuery
    runs-on: ubuntu-latest
    needs: [determine_tests_to_run, check_previous_pass]
    if: ${{ fromJSON(needs.check_previous_pass.outputs.successfully_passed) == false && (needs.determine_tests_to_run.outputs.run_replica == 'true' || needs.determine_tests_to_run.outputs.run_generic_src == 'true') }}
    outputs:
      bigquery_credentials: ${{ steps.authenticate.outputs.bigquery_credentials || secrets.BIGQUERY_CREDENTIALS }}
    env:
      BIGQUERY_CREDENTIALS: ${{ secrets.BIGQUERY_CREDENTIALS }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # if the BIGQUERY_CREDENTIALS secret is missing, run an action that prompts for credentials
      - name: Authenticate Google BigQuery
        id: authenticate
        if: ${{ env.BIGQUERY_CREDENTIALS == '' }}
        uses: ./.github/actions/authenticate-gbq
        with:
          timeout: 300 # 5 minutes

  test__greenlink_gtfs:
    name: "Test / Greenlink GTFS"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run]
    if: needs.determine_tests_to_run.outputs.run_greenlink_gtfs == 'true'
    env:
      CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          artifact_name: "data__greenlink_gtfs"
          etls: "greenlink_gtfs"

  test__greenlink_ridership:
    name: "Test / Greenlink Ridership"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run]
    if: needs.determine_tests_to_run.outputs.run_greenlink_ridership == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Create sample input data
        env:
          CSV1_CONTENT: |
            ,Period,Stop Point,Ridership - Boarding - APC,Ridership - Alighting - APC
            1,7/1/2024,1000,1287,1291
            2,7/2/2024,1000,1238,1250
            2,7/4/2024,1010,
            3,7/3/2024,1000,1403,1353
            2,7/5/2024,1010,"non-numeric value",1234
            2,7/6/2024,1010,1234,"2"
          CSV2_CONTENT: |
            ,Period,Stop Point,Ridership - Boarding - APC,Ridership - Alighting - APC,extra column
            1,1/2/2025,1000,1354,1333,extra value
            2,1/3/2025,1000,1384,1306,"extra value with ""quotes"""
        run: |
          mkdir -p ${{ github.workspace }}/pipeline-input/greenlink_ridership

          cat <<EOF > "${{ github.workspace }}/pipeline-input/greenlink_ridership/file1.csv"
          ${{ env.CSV1_CONTENT }}
          EOF

          cat <<EOF > "${{ github.workspace }}/pipeline-input/greenlink_ridership/file2.csv"
          ${{ env.CSV2_CONTENT }}
          EOF

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          artifact_name: "data__greenlink_ridership"
          etls: "greenlink_gtfs,greenlink_ridership"
          input_dir: "${{ github.workspace }}/pipeline-input"

  test__census_acs_5year:
    name: "Test / Census ACS 5-Year Estimates"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run]
    if: needs.determine_tests_to_run.outputs.run_census_acs_5year == 'true'
    env:
      CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          bigquery_credentials: ${{ needs.authenticate_google_big_query.outputs.bigquery_credentials }}
          artifact_name: "data__census_acs_5year"
          etls: "census_acs_5year"

  test__replica:
    name: "Test / Replica"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run, authenticate_google_big_query]
    if: needs.determine_tests_to_run.outputs.run_replica == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Create sample input data
        env:
          GEOJSON: '{"type":"FeatureCollection","features":[{"type":"Feature","properties":{},"geometry":{"type":"Polygon","coordinates":[[[-82.4,34.852],[-82.4,34.85],[-82.398,34.85],[-82.398,34.852],[-82.4,34.852]]]}}]}'
          AREA_GEOJSON: '{"type":"FeatureCollection","features":[{"type":"Feature","properties":{},"geometry":{"type":"Polygon","coordinates":[[[-82.4,34.852],[-82.4,34.85],[-82.398,34.85],[-82.398,34.852],[-82.4,34.852]]]}}]}'
        run: |
          mkdir -p ${{ github.workspace }}/replica-input/replica_interest_area_polygons
          echo $AREA_GEOJSON > ${{ github.workspace }}/replica-input/replica_interest_area_polygons/full_area.geojson
          echo $GEOJSON > ${{ github.workspace }}/replica-input/replica_interest_area_polygons/sample.geojson

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          bigquery_credentials: ${{ needs.authenticate_google_big_query.outputs.bigquery_credentials }}
          artifact_name: "data__replica"
          input_dir: "${{ github.workspace }}/replica-input"
          etls: "greenlink_gtfs,replica"

  test__geocoder:
    name: "Test / Geocoder"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run]
    if: needs.determine_tests_to_run.outputs.run_geocoder == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Create sample input data
        env:
          CSV1_CONTENT: |
            Company Name,Address,City,State,ZIP Code,ZIP Four,County,Metro Area,Neighborhood
            7-Eleven,7 Eleven,Fountain Inn,SC,29644,,Greenville,"Greenville, SC",
            7-Eleven,1703 Easley Bridge Rd,Greenville,SC,29611,5320,Greenville,"Greenville, SC",
            7-Eleven,2617 Wade Hampton Blvd,Greenville,SC,29615,1149,Greenville,"Greenville, SC",
        run: |
          mkdir -p ${{ github.workspace }}/pipeline-input/to_geocode

          cat <<EOF > "${{ github.workspace }}/pipeline-input/to_geocode/file1.csv"
          ${{ env.CSV1_CONTENT }}
          EOF

      - name: Run test (with data)
        uses: ./.github/actions/prepare-pipeline-test
        with:
          artifact_name: "data__geocoder"
          etls: "geocoder"
          input_dir: "${{ github.workspace }}/pipeline-input"

      - name: Delete input data
        run: |
          rm -rf ${{ github.workspace }}/pipeline-input/to_geocode
          mkdir -p ${{ github.workspace }}/pipeline-input/to_geocode

      - name: Run test (no data)
        uses: ./.github/actions/prepare-pipeline-test
        with:
          artifact_name: "data__geocoder_nodata"
          etls: "geocoder"
          input_dir: "${{ github.workspace }}/pipeline-input"

  test__essential_services:
    name: "Test / Essential Services"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run]
    if: needs.determine_tests_to_run.outputs.run_essential_services == 'true'
    env:
      CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Create sample input data
        env:
          CSV1_CONTENT: |
            Company Name,Address,City,State,ZIP Code,ZIP Four,County,Metro Area,Neighborhood
            7-Eleven,7 Eleven,Fountain Inn,SC,29644,,Greenville,"Greenville, SC",
            7-Eleven,1703 Easley Bridge Rd,Greenville,SC,29611,5320,Greenville,"Greenville, SC",
            7-Eleven,2617 Wade Hampton Blvd,Greenville,SC,29615,1149,Greenville,"Greenville, SC",
        run: |
          mkdir -p ${{ github.workspace }}/pipeline-input/to_geocode

          cat <<EOF > "${{ github.workspace }}/pipeline-input/to_geocode/grocery_stores__2025-07-18.csv"
          ${{ env.CSV1_CONTENT }}
          EOF

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          artifact_name: "data__essential_services"
          etls: "geocoder,greenlink_gtfs,replica,essential_services"
          input_dir: "${{ github.workspace }}/pipeline-input"

  test__all:
    name: "Test / All ETLs"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run, authenticate_google_big_query]
    if: needs.determine_tests_to_run.outputs.run_generic_src == 'true'
    env:
      CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Create sample input data
        env:
          GEOJSON: '{"type":"FeatureCollection","features":[{"type":"Feature","properties":{},"geometry":{"type":"Polygon","coordinates":[[[-82.4,34.852],[-82.4,34.85],[-82.398,34.85],[-82.398,34.852],[-82.4,34.852]]]}}]}'
          AREA_GEOJSON: '{"type":"FeatureCollection","features":[{"type":"Feature","properties":{},"geometry":{"type":"Polygon","coordinates":[[[-82.4,34.852],[-82.4,34.85],[-82.398,34.85],[-82.398,34.852],[-82.4,34.852]]]}}]}'
          CSV1_CONTENT: |
            ,Period,Stop Point,Ridership - Boarding - APC,Ridership - Alighting - APC
            1,7/1/2024,1000,1287,1291
            2,7/2/2024,1000,1238,1250
            3,7/3/2024,1000,1403,1353
          CSV2_CONTENT: |
            ,Period,Stop Point,Ridership - Boarding - APC,Ridership - Alighting - APC,extra column
            1,1/2/2025,1000,1354,1333,extra value
            2,1/3/2025,1000,1384,1306,"extra value with ""quotes"""
        run: |
          mkdir -p ${{ github.workspace }}/pipeline-input/replica_interest_area_polygons
          mkdir -p ${{ github.workspace }}/pipeline-input/greenlink_ridership
          mkdir -p ${{ github.workspace }}/pipeline-input/passthrough_data

          # write sample data for replica
          echo $AREA_GEOJSON > ${{ github.workspace }}/pipeline-input/replica_interest_area_polygons/full_area.geojson
          echo $GEOJSON > ${{ github.workspace }}/pipeline-input/replica_interest_area_polygons/sample.geojson

          # write sample data for greenlink ridership
          cat <<EOF > "${{ github.workspace }}/pipeline-input/greenlink_ridership/file1.csv"
          ${{ env.CSV1_CONTENT }}
          EOF
          cat <<EOF > "${{ github.workspace }}/pipeline-input/greenlink_ridership/file2.csv"
          ${{ env.CSV2_CONTENT }}
          EOF

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          bigquery_credentials: ${{ needs.authenticate_google_big_query.outputs.bigquery_credentials }}
          artifact_name: "data__all"
          input_dir: "${{ github.workspace }}/pipeline-input"

  check_previous_pass:
    name: Check Previous Pass
    runs-on: ubuntu-latest
    needs: [determine_tests_to_run]
    if: ${{ fromJSON(needs.determine_tests_to_run.outputs.no_changes) != true }}
    env:
      TESTS_TO_RUN_HASH: ${{ needs.determine_tests_to_run.outputs.output_hash }}

    outputs:
      successfully_passed: ${{ steps.check.outputs.successfully_passed }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Check if ACT_SUCCESS_COMMIT_HASH variable exists
        id: check
        run: |
          if [ -n "${{ github.head_ref }}" ]; then
            LOCAL_REF_NAME="${{ github.head_ref }}"
          else
            LOCAL_REF_NAME=$(git rev-parse --abbrev-ref HEAD)
          fi

          # sanitize branch name: 
          SANITIZED_BRANCH_NAME=${LOCAL_REF_NAME// /} # remove whitespace
          SANITIZED_BRANCH_NAME=${SANITIZED_BRANCH_NAME//-/_} # replace '-' with '_'
          SANITIZED_BRANCH_NAME=$(echo "$SANITIZED_BRANCH_NAME" | tr '[:lower:]' '[:upper:]')

          BRANCH_VAR_NAME="ACT_SUCCESS_COMMIT_HASH__${SANITIZED_BRANCH_NAME}"

          # get the vars that are set in the repository
          VARS_JSON="${{ toJSON(vars) }}"

          # ensure values are quoted
          VARS_JSON=$(echo "$VARS_JSON" | sed -E 's/([[:alnum:]_]+): ([^,}[:space:]]+)/\1:"\2"/g' | sed -E 's/([[:alnum:]_]+):/"\1":/g')

          # get the value of the variable for the current branch
          LAST_PASS_HASH=$(echo "$VARS_JSON" | jq -r --arg key "$BRANCH_VAR_NAME" '.[$key]')

          # hash the contents of the current branch (this includes any uncommitted changes when running locally)
          CURRENT_HASH=$(find . -type f -not -path "*/.vscode/*" -not -path "*/.git/*" -not -path "./data-pipeline/data/*" -not -path "./data-pipeline/env/*" -not -path "./data-pipeline/input/*" \( -exec sha1sum "$PWD"/{} \; \) | awk '{print $1}' | sort | sha1sum | cut -d ' ' -f 1)
          CURRENT_HASH="$CURRENT_HASH$TESTS_TO_RUN_HASH" # append the hash of the tests to run to the current hash

          # check whether it matches the current commit hash
          if [ "$LAST_PASS_HASH" == "$CURRENT_HASH" ]; then
            echo "$BRANCH_VAR_NAME variable exists and matches the current commit hash."
            echo "successfully_passed=true" >> "$GITHUB_OUTPUT"
          else
            echo "$BRANCH_VAR_NAME variable does not exist or does not match the current commit hash."
            echo "LAST_PASS_HASH: $LAST_PASS_HASH"
            echo "CURRENT_HASH: $CURRENT_HASH"
            echo "successfully_passed=false" >> "$GITHUB_OUTPUT"
          fi

  update_pass_variable:
    name: "Save Run Success"
    runs-on: ubuntu-latest
    needs:
      [
        determine_tests_to_run,
        check_previous_pass,
        build,
        test__greenlink_gtfs,
        test__greenlink_ridership,
        test__census_acs_5year,
        test__replica,
        test__geocoder,
        test__essential_services,
        test__all,
      ]
    if: ${{ !cancelled() && fromJSON(needs.determine_tests_to_run.outputs.no_changes) != true && fromJSON(needs.check_previous_pass.outputs.successfully_passed) != true }} # always run even if any of the pipeline-related jobs in needs are skipped (skip if there were no tests to run)
    env:
      TESTS_TO_RUN_HASH: ${{ needs.determine_tests_to_run.outputs.output_hash }}

    steps:
      - name: Fail if any job in needs failed
        if: ${{ contains(needs.*.result, 'failure') }}
        run: |
          echo "One or more jobs in needs failed. Failing the workflow."
          exit 1

      - name: Checkout code
        uses: actions/checkout@v3

      - name: Install GitHub CLI
        if: "${{ env.ACT }}"
        run: |
          sudo apt-get update
          sudo apt-get install -y gh

      - name: Update ACT_SUCCESS_COMMIT_HASH (on success)
        if: "${{ env.ACT }}"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ -n "${{ github.head_ref }}" ]; then
            LOCAL_REF_NAME="${{ github.head_ref }}"
          else
            LOCAL_REF_NAME=$(git rev-parse --abbrev-ref HEAD)
          fi

          # sanitize branch name: 
          SANITIZED_BRANCH_NAME=${LOCAL_REF_NAME// /} # remove whitespace
          SANITIZED_BRANCH_NAME=${SANITIZED_BRANCH_NAME//-/_} # replace '-' with '_'
          SANITIZED_BRANCH_NAME=$(echo "$SANITIZED_BRANCH_NAME" | tr '[:lower:]' '[:upper:]')

          BRANCH_VAR_NAME="ACT_SUCCESS_COMMIT_HASH__${SANITIZED_BRANCH_NAME}"

          # hash the contents of the current branch (this includes any uncommitted changes when running locally)
          CURRENT_HASH=$(find . -type f -not -path "*/.vscode/*" -not -path "*/.git/*" -not -path "./data-pipeline/data/*" -not -path "./data-pipeline/env/*" -not -path "./data-pipeline/input/*" \( -exec sha1sum "$PWD"/{} \; \) | awk '{print $1}' | sort | sha1sum | cut -d ' ' -f 1)
          CURRENT_HASH="$CURRENT_HASH$TESTS_TO_RUN_HASH" # append the hash of the tests to run to the current hash

          echo "Setting Repository Variable: $BRANCH_VAR_NAME to $CURRENT_HASH"

          # check if the variable already exists
          EXISTING_VAR=$(gh api -H "Accept: application/vnd.github.v3+json" /repos/${{ github.repository }}/actions/variables/$BRANCH_VAR_NAME --silent 2>/dev/null || echo "null")

          if [ "$EXISTING_VAR" != "null" ]; then
            echo "Repository variable $BRANCH_VAR_NAME already exists. Updating it."
            gh api \
              --method PATCH \
              -H "Accept: application/vnd.github.v3+json" \
              /repos/${{ github.repository }}/actions/variables/$BRANCH_VAR_NAME \
              -f value="$CURRENT_HASH"
          else
            echo "Repository variable $BRANCH_VAR_NAME does not exist. Creating it."
            gh api \
              --method POST \
              -H "Accept: application/vnd.github.v3+json" \
              /repos/${{ github.repository }}/actions/variables \
              -f name=$BRANCH_VAR_NAME \
              -f value="$CURRENT_HASH"
          fi

          echo "Repository variable updated successfully."
