name: "Data Pipeline (PR)"

on:
  pull_request:
    branches:
      - main
    paths:
      - "data-pipeline/**"
      - "gbq-auth/**"

jobs:
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [determine_tests_to_run]

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Set up buildx cache directory
        run: |
          mkdir -p /tmp/.buildx-cache

      - name: Restore Docker build cache
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-docker-${{ hashFiles('./data-pipeline/Dockerfile', './data-pipeline/environment.yaml', './data-pipeline/src/**') }}
          restore-keys: |
            ${{ runner.os }}-docker-

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: ./data-pipeline
          file: ./data-pipeline/Dockerfile
          push: false # don't push the image to the registry
          load: true # instead, load the image into the daemon
          tags: gc-mobility-dashboard-data-pipeline:test
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache,mode=max

      - name: Verify Built Images
        run: |
          docker images

      - name: Save Docker image as artifact
        run: |
          docker save gc-mobility-dashboard-data-pipeline:test -o ./gc-mobility-dashboard-data-pipeline.tar

      - name: Upload Docker image artifact
        uses: actions/upload-artifact@v4
        with:
          name: docker-image-data-pipeline
          path: ./gc-mobility-dashboard-data-pipeline.tar
          retention-days: 1 # set a low retention for temporary artifacts

  determine_tests_to_run:
    name: Determine Tests to Run
    runs-on: ubuntu-latest
    outputs:
      run_greenlink_gtfs: ${{ steps.check_changed_dirs.outputs.run_greenlink_gtfs }}
      run_census_acs_5year: ${{ steps.check_changed_dirs.outputs.run_census_acs_5year }}
      run_replica: ${{ steps.check_changed_dirs.outputs.run_replica }}
      run_generic_src: ${{ steps.check_changed_dirs.outputs.run_generic_src }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0 # fetch all history to ensure we can find the merge base

      - name: Determine changed directories
        id: check_changed_dirs
        run: |
          # Fetch the base branch to ensure the merge-base can be found
          git fetch origin ${{ github.event.pull_request.base.ref }}:${{ github.event.pull_request.base.ref }}
          MERGE_BASE=$(git merge-base origin/${{ github.event.pull_request.base.ref }} HEAD)
          echo "Merge base: $MERGE_BASE"
          echo "HEAD: $(git rev-parse HEAD)"
          echo "Base Ref: ${{ github.event.pull_request.base.ref }}"
          echo "Origin Base Ref SHA: $(git rev-parse origin/${{ github.event.pull_request.base.ref }})"

          # Get a list of changed files between the merge base and the head of the PR
          # Filter to only files within data-pipeline/src
          CHANGED_FILES=$(git diff --name-only $(git merge-base origin/${{ github.event.pull_request.base.ref }} HEAD) HEAD -- ./data-pipeline/src/)

          RUN_GREENLINK_GTFS_TEST="false"
          RUN_CENSUS_ACS_5YEAR_TEST="false"
          RUN_REPLICA_TEST="false"
          RUN_GENERIC_SRC_TEST="false"

          if [ -n "$CHANGED_FILES" ]; then
            echo "Changed files in data-pipeline/src:"
            echo "$CHANGED_FILES"

            for file in $CHANGED_FILES; do
              if [[ "$file" == "data-pipeline/src/etl/sources/greenlink_gtfs/"* ]]; then
                RUN_GREENLINK_GTFS_TEST="true"
              elif [[ "$file" == "data-pipeline/src/etl/sources/census_acs_5year/"* ]]; then
                RUN_CENSUS_ACS_5YEAR_TEST="true"
              elif [[ "$file" == "data-pipeline/src/etl/sources/replica/"* ]]; then
                RUN_REPLICA_TEST="true"
              elif [[ "$file" == "data-pipeline/src/"* ]] && [[ "$file" != "data-pipeline/src/etl/sources/"* ]]; then
                RUN_GENERIC_SRC_TEST="true"
              fi
            done

            # if the generic source test is true, do not trigger the specific tests
            # because the generic test will cover them
            if [ "$RUN_GENERIC_SRC_TEST" == "true" ]; then
                RUN_GREENLINK_GTFS_TEST="false"
                RUN_CENSUS_ACS_5YEAR_TEST="false"
                RUN_REPLICA_TEST="false"
            fi

          else
            echo "No changes detected in data-pipeline/src."
          fi

          echo "run_greenlink_gtfs=$RUN_GREENLINK_GTFS_TEST" >> "$GITHUB_OUTPUT"
          echo "run_census_acs_5year=$RUN_CENSUS_ACS_5YEAR_TEST" >> "$GITHUB_OUTPUT"
          echo "run_replica=$RUN_REPLICA_TEST" >> "$GITHUB_OUTPUT"
          echo "run_generic_src=$RUN_GENERIC_SRC_TEST" >> "$GITHUB_OUTPUT"

          # print the results
          echo "Run Greenlink GTFS: $RUN_GREENLINK_GTFS_TEST"
          echo "Run Census ACS 5-Year: $RUN_CENSUS_ACS_5YEAR_TEST"
          echo "Run Replica: $RUN_REPLICA_TEST"
          echo "Run all: $RUN_GENERIC_SRC_TEST"

  authenticate_google_big_query:
    name: Authenticate Google BigQuery
    runs-on: ubuntu-latest
    needs: [determine_tests_to_run]
    if: needs.determine_tests_to_run.outputs.run_replica == 'true' || needs.determine_tests_to_run.outputs.run_generic_src == 'true'
    outputs:
      bigquery_credentials: ${{ steps.authenticate.outputs.bigquery_credentials }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate Google BigQuery
        id: authenticate
        uses: ./.github/actions/authenticate-gbq
        with:
          timeout: 300 # 5 minutes

  test__greenlink_gtfs:
    name: "Test / Greenlink GTFS"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run]
    if: needs.determine_tests_to_run.outputs.run_greenlink_gtfs == 'true'
    env:
      CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          bigquery_credentials: ${{ needs.authenticate_google_big_query.outputs.bigquery_credentials }}
          artifact_name: "data__greenlink_gtfs"
          etls: "replica"

  test__census_acs_5year:
    name: "Test / Census ACS 5-Year Estimates"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run]
    if: needs.determine_tests_to_run.outputs.run_census_acs_5year == 'true'
    env:
      CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          bigquery_credentials: ${{ needs.authenticate_google_big_query.outputs.bigquery_credentials }}
          artifact_name: "data__census_acs_5year"
          etls: "replica"

  test__replica:
    name: "Test / Replica"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run, authenticate_google_big_query]
    if: needs.determine_tests_to_run.outputs.run_replica == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Create sample input data
        env:
          GEOJSON: '{"type":"FeatureCollection","features":[{"type":"Feature","properties":{},"geometry":{"type":"Polygon","coordinates":[[[-82.4,34.852],[-82.4,34.85],[-82.398,34.85],[-82.398,34.852],[-82.4,34.852]]]}}]}'
        run: |
          mkdir -p ${{ github.workspace }}/replica-input/replica_interest_area_polygons
          echo $GEOJSON > ${{ github.workspace }}/replica-input/replica_interest_area_polygons/sample.geojson

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          bigquery_credentials: ${{ needs.authenticate_google_big_query.outputs.bigquery_credentials }}
          artifact_name: "data__replica"
          input_dir: "${{ github.workspace }}/replica-input"
          etls: "replica"

  test__all:
    name: "Test / All ETLs"
    runs-on: ubuntu-latest
    needs: [build, determine_tests_to_run, authenticate_google_big_query]
    if: needs.determine_tests_to_run.outputs.run_generic_src == 'true'
    env:
      CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Create sample input data
        env:
          GEOJSON: '{"type":"FeatureCollection","features":[{"type":"Feature","properties":{},"geometry":{"type":"Polygon","coordinates":[[[-82.4,34.852],[-82.4,34.85],[-82.398,34.85],[-82.398,34.852],[-82.4,34.852]]]}}]}'
        run: |
          mkdir -p ${{ github.workspace }}/replica-input/replica_interest_area_polygons
          echo $GEOJSON > ${{ github.workspace }}/replica-input/replica_interest_area_polygons/sample.geojson

      - name: Run test
        uses: ./.github/actions/prepare-pipeline-test
        with:
          bigquery_credentials: ${{ needs.authenticate_google_big_query.outputs.bigquery_credentials }}
          artifact_name: "data__all"
          input_dir: "${{ github.workspace }}/replica-input"
